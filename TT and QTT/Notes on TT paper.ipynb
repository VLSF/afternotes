{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN061C4sSGmnjogIqVhuFWH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","I. V. Oseledets introduced the TT format in the article Tensor-Train Decomposition https://doi.org/10.1137/090752286. Here, I summarise a theoretical part of this paper (the existence of TT and constructive results on how to build TT with SVD and so on) as I understand it.\n","\n","For a $d$ dimensional tensor, TT decomposition reads\n","$$A(i_1, \\dots, i_d) = \\sum_{\\alpha_0,\\dots,\\alpha_{d+1}} G_{1}(\\alpha_0,i_1,\\alpha_1)\\cdots G_{1}(\\alpha_{d},i_{d},\\alpha_{d+1}),\\,\\alpha_i=1,\\dots r_i,\\,r_0=r_{d+1} = 1,$$\n","so we only need to store cores (carriages) $G_i$ which requires $\\leq d \\left(\\max(r_i)\\right)^2$ floats."],"metadata":{"id":"hyZBeRFDoPJa"}},{"cell_type":"markdown","source":["# Existence\n","It is easy to show that tensor train decomposition exists and find TT ranks $r_k$ with SVD.\n","The main techniques here are\n","1. Unfolding\n","2. SVD\n","The unfolding of a tensor is a standard reshape operation (`A.reshape(m, n)`), where tensor indices are mapped on indices for matrix rows and columns. Symbolically this is written as follows\n","$$A_k = A(i_1,\\dots, i_k;i_{k+1},\\dots,i_{d}),$$\n","i.e., indices before semicolon mark rows and indices after mark column of matrix $A_k$.\n","\n","**Theorem 2.1**\n","\n","If each unfolding of $A_k$ has rank $r_k$, there exists a TT decomposition with $k$-th TT-ranks $\\leq r_k$.\n","\n","**Proof**\n","\n","The proof is by induction.\n","Consider the first unfolding $A_1\\in\\mathbb{R}^{n_1\\times m_1}$. Since this is an ordinary matrix, it can be presented as a product of two matrices (e.g., using SVD) with contraction dimension of size $r_1$, i.e.,\n","$$A_1 = \\underset{n_1\\times r_1}{U}\\,\\underset{r_1\\times m_1}{V^{T}}.$$\n","If we restore indices, the identity above reads\n","$$A(i_1,\\dots,i_k) = \\sum_{\\alpha}U(i_1,\\alpha) V(\\alpha,i_2,\\dots, i_d),$$\n","so $U(i_1, \\alpha)$ is a legitimate first TT core. To proceed, we need to show that tensor $V$ has the same property that all its unfoldings $k=2,\\dots, d$ have ranks $r_k$.\n","To show that we use\n","$$V = A_1^{T}U \\left(U^T U\\right)^{-1} = A_1^{T}W \\Leftrightarrow V(\\alpha,i_2,\\dots, i_d) = \\sum_{i_1}A(i_1, \\dots, i_d)W(i_1,\\alpha),$$\n","and the fact that $k$-th unfoldings of $A$ has rank $r_k$, that is,\n","$$A(i_1, \\dots, i_d) = \\sum_{\\alpha = 1}^{r_k} X(i_1, \\dots, i_k, \\alpha) Y(\\alpha, i_{k+1},\\dots, i_{d}).$$\n","Combining these two facts, we obtain\n","$$V(\\alpha, i_2, \\dots, i_d) = \\sum_{\\beta}\\widetilde{X}(\\alpha, i_2, \\dots, i_k, \\beta) Y(\\beta, i_{k+1},\\dots, i_{d}),$$\n","so indeed $k$-th unfolding of $V$ has rank $r_k$.\n","To continue the induction process, we reshape $V$ and again consider the first unfolding\n","$$V(\\alpha i_2, i_3, \\dots, i_d) = \\sum_{\\beta} U(\\alpha i_2, \\beta)V^{'}(\\beta, t_3,\\dots, i_d) = \\sum_{\\beta} G(\\alpha, i_2, \\beta) V^{'}(\\beta, t_3,\\dots, i_d).$$\n","The proof is constructive, so it is not hard to deduce an SVD-based algorithm for the sequential computation of tensor cores."],"metadata":{"id":"ww_l3QH4oSGe"}},{"cell_type":"markdown","source":["# Approximate TT\n","\n","**Theorem 2.2.**\n","\n","Suppose that unfoldings are only approximately low-rank, that is, \n","$$A_k = R_k + E_k,\\,\\text{rank }R_k = r_k,\\,\\left\\|E_k\\right\\|_F\\leq \\epsilon_k.$$\n","\n","In this case, TT-SVD algorithm (implicitly given in the existence part) constructs approximate tensor $B$ such, that\n","$$\\left\\| A - B\\right\\|_F \\leq \\sqrt{\\sum_{k=1}^{d-1}\\epsilon_k^2}.$$\n","\n","**Proof**\n","\n","This proof is by induction in the number of dimensions.\n","For $d=2$, everything is known since TT-SVD is SVD.\n","Consider the first unfolding for $d > 2$ and use SVD to find\n","$$A_1 = U_1 B_1 + E_1.$$\n","Now, tensor $B_1$ (considered as tensor with $d$ dimensions with first and second indices merged) will be approximated by TT-SVD with a different tensor $\\hat{B}_1$. So, for the distance between the original tensor and the approximate one, we get\n","$$\\left\\|A - B\\right\\|_F^2 = \\left\\|A_1 - U_1\\hat{B}\\right\\|_F^2 = \\left\\|A_1 - U_1\\hat{B}_1 - U_1 B +U_1 B_1\\right\\|_F^2 = \\left\\|E_1 + U_1(B_1 - \\hat{B}_1)\\right\\|_F^2$$\n","$$=\\left\\|E_1\\right\\|_F^2 + \\left\\|B_1 - \\hat{B}_1\\right\\|_{F}^2,$$\n","where we used $U_1^T U_1 = I$ and $U_1^T E_1 = 0$, which is direct consequence of SVD used to construct $U_1$, $B_1$ and $E_1$.\n","We can proceed by induction if $B_1$ has the same property that $A$, i.e., its remaining unfoldings have approximate rank $r_k$. Let's show that this is the case.\n","Observe that\n","$$B_1 = U_1^T A_1 \\Leftrightarrow B(\\alpha i_{2},\\dots i_{d}) = \\sum_{i_1} U_1(\\alpha, i_1) A(i_1, \\dots, i_d).$$\n","Next, we use that each unfolding of $A_k$ is approximately of rank $r_k$, i.e.,\n","$$A(i_1, \\dots, i_d) = \\sum_{\\beta = 1}^{r_k} H(i_1, \\dots, i_k, \\beta) D(\\beta, i_{k+1},\\dots, i_{d}) + E_k(i_1, \\dots, i_d),\\,\\left\\|E_k\\right\\|_F \\leq \\epsilon_k.$$\n","Inserting this identity into the definition of $B_1$, we get\n","$$B(\\alpha i_{2},\\dots i_{d}) = \\sum_{i_1} U_1(\\alpha, i_1) \\left(\\sum_{\\beta = 1}^{r_k} H(i_1, \\dots, i_k, \\beta) D(\\beta, i_{k+1},\\dots, i_{d}) + E_k(i_1, \\dots, i_d)\\right).$$\n","The first sum is of the correct structure, so we only need to show the Frobenius norm of the reminder $\\leq \\epsilon_{k}$. To show that, we consider the Frobenius norm of the first unfolding of $E_{k}$. This gives us\n","$$\\left\\|\\sum_{i_1} U_1(\\alpha, i_1)E_k(i_1, \\dots, i_d)\\right\\|_F = \\left\\|U_1 \\left(E_k\\right)_1\\right\\|_F = \\left\\|\\left(E_k\\right)_1\\right\\|_F = \\left\\|E_k(i_1, \\dots, i_d)\\right\\|_F = \\epsilon_k.$$\n","Since $B_1$ has the same properties as $A$ but is a tensor with $d-1$ dimensions, we are done with the induction step and have\n","$$\\left\\|A - B\\right\\|_F^2 \\leq \\sum_{i=1}^{d-1}\\epsilon_i^2.$$\n"],"metadata":{"id":"XInDZkieoKUH"}},{"cell_type":"markdown","source":["# Algorithm\n","The proofs wer constructive, so TT-SVD algorithm is easy to recover. In the simplest form when we can store the whole tensor in memory, it can be realized with SVD and reshapes.\n","\n","**Require:** tensor $A$ having shapes $\\left(n_1, \\dots, n_d\\right)$, required relative accuracy $\\epsilon$\n","\n","**Init:** find truncation parameters $\\delta = \\epsilon \\left\\|A\\right\\|_F \\big/ \\sqrt{d-1}$, set $r_0 = 1$, $C = A$\n","\n","for $k = 1,\\dots, d-1$:\n","\n","> $C = C.\\text{reshape}(r_{k-1}n_k, |C| - r_{k-1}n_k)$\n","> \n","> Use SVD to find low-rank apparoximation $C = U S V^{T} + E,\\,\\left\\|E\\right\\|_F \\leq \\delta$ \n","> \n","> Set $r_k = \\text{rank}(U)$\n","> \n","> Form TT core $G_k = U.\\text{reshape}(r_{k-1}, n_k, r_{k})$\n","> \n","> Set $C = S V^{T} = U^T C$ \n","\n","$G_d = C$"],"metadata":{"id":"a9qHv0R_obKO"}},{"cell_type":"markdown","source":["# Operations in TT format\n","To do\n","\n","## Addition\n","\n","## Hadamard product\n","\n","## Scalar product\n","\n","## Matrix product\n"],"metadata":{"id":"TrWalImroczo"}}]}